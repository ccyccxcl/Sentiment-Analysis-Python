{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "导入所需模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8  -*-\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import jieba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 5)\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "数据准备，读取文件，数据预处理阶段包括：整合数据生成样本空间、生成词袋、统计词频生成模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding=utf-8 \n",
    "import os\n",
    "#获取目标文件夹的路径\n",
    "filedir ='C:\\\\Users\\\\ccy\\\\Desktop\\\\ChnSentiCorp\\\\ChnSentiCorp\\\\neg'\n",
    "#获取当前文件夹中的文件名称列表  \n",
    "filenames=os.listdir(filedir)\n",
    "#打开当前目录下的result.txt文件，如果没有则创建\n",
    "f=open('C:\\\\Users\\\\ccy\\\\Desktop\\\\ChnSentiCorp\\\\ChnSentiCorp\\\\neg.txt','w')\n",
    "#先遍历文件名\n",
    "for filename in filenames:\n",
    "    filepath = filedir+'/'+filename\n",
    "    #遍历单个文件，读取行数\n",
    "    for line in open(filepath,encoding='UTF-8'):\n",
    "        f.writelines(line)\n",
    "        f.write('\\n')\n",
    "#关闭文件\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#准备停用词表\n",
    "stopwords = []\n",
    "for line in open(\"E:/Jupyter/sentiment/stopwords.txt\",encoding=\"UTF-8\"):\n",
    "    stopwords.append(line.strip())\n",
    "#生成词袋    \n",
    "def read_file(fi,sentiment, stopwords, words, sentences):\n",
    "    for line in open(fi,encoding=\"UTF-8\"):\n",
    "        try:\n",
    "            segs = jieba.lcut(line.strip())#结巴分词\n",
    "            segs = [word for word in segs if word not in stopwords and len(word) > 1]#去掉停用词\n",
    "            words.extend(segs)\n",
    "            sentences.append((segs, sentiment)) # tuple，评价+标签\n",
    "        except:\n",
    "            print(line)\n",
    "            continue\n",
    "words = []\n",
    "sentences = []\n",
    "sentiment = 1\n",
    "read_file('E:/Jupyter/sentiment/pos.txt', sentiment, stopwords, words, sentences)#正向评价及标签\n",
    "sentiment = 0\n",
    "read_file('E:/Jupyter/sentiment/neg.txt', sentiment, stopwords, words, sentences)#负向评价及标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "机器学习方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8266666666666667"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#整合正负向语料\n",
    "x, y = zip(*sentences) #zip相当与压缩 ，zip（*）相当于解压\n",
    "x = [\" \".join(sentences) for sentences in x]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1, test_size=0.1)  #划分训练集和测试集\n",
    "# 特征提取\n",
    "vec = CountVectorizer(ngram_range=(1, 2), max_features= 1000) #ngram_range=(1, 2)选用1,2个词进行前后的组合，构成新的标签值，\n",
    "#CountVectorizer只考虑每种词汇在该训练文本中出现的频。将文本向量转换成稀疏表示数值向量（字符频率向量）。max_feature 选取频率高的单词\n",
    "vec.fit(x_train)\n",
    "classifier = MultinomialNB() #基于多项式的朴素贝叶\n",
    "classifier.fit(vec.transform(x_train), y_train) #tranform()的作用是通过找中心和缩放等实现标准化\n",
    "# 测试得分\n",
    "classifier.score(vec.transform(x_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5400/5400 [==============================] - 163s 30ms/step - loss: 0.5569 - acc: 0.7065\n",
      "Epoch 2/10\n",
      "5400/5400 [==============================] - 157s 29ms/step - loss: 0.3255 - acc: 0.8678\n",
      "Epoch 3/10\n",
      "5400/5400 [==============================] - 177s 33ms/step - loss: 0.2604 - acc: 0.9004\n",
      "Epoch 4/10\n",
      "5400/5400 [==============================] - 168s 31ms/step - loss: 0.2137 - acc: 0.9213\n",
      "Epoch 5/10\n",
      "5400/5400 [==============================] - 163s 30ms/step - loss: 0.1821 - acc: 0.9341\n",
      "Epoch 6/10\n",
      "5400/5400 [==============================] - 160s 30ms/step - loss: 0.1606 - acc: 0.9407\n",
      "Epoch 7/10\n",
      "5400/5400 [==============================] - 165s 31ms/step - loss: 0.1356 - acc: 0.9491\n",
      "Epoch 8/10\n",
      "5400/5400 [==============================] - 165s 31ms/step - loss: 0.1180 - acc: 0.9572\n",
      "Epoch 9/10\n",
      "5400/5400 [==============================] - 158s 29ms/step - loss: 0.1085 - acc: 0.9576\n",
      "Epoch 10/10\n",
      "5400/5400 [==============================] - 160s 30ms/step - loss: 0.1024 - acc: 0.9619\n",
      "Logloss损失: 0.52\n",
      "验证集的准确率 :0.85\n"
     ]
    }
   ],
   "source": [
    "#深度学习\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=2500, split=' ') # 用于向量化文本,或将文本转换为序列，保留2500个词\n",
    "tokenizer.fit_on_texts(x)#生成词典\n",
    "X = tokenizer.texts_to_sequences(x)#将每条文本转变成一个向量\n",
    "X = pad_sequences(X)#将序列转化为经过填充以后的一个长度相同的新序列\n",
    "\n",
    "# 设定embedding维度等超参数\n",
    "embed_dim = 16\n",
    "lstm_out = 100\n",
    "batch_size = 32\n",
    "\n",
    "# 构建LSTM网络\n",
    "model = Sequential()\n",
    "model.add(Embedding(2500, embed_dim, input_length=X.shape[1], dropout=0.2))\n",
    "model.add(LSTM(lstm_out, dropout_U=0.2, dropout_W=0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "Y = pd.get_dummies(pd.DataFrame({'label': [str(target) for target in y]})).values\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, test_size=0.1, random_state=7)\n",
    "\n",
    "# 拟合与训练模型\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=10)\n",
    "\n",
    "# 验证\n",
    "score, acc = model.evaluate(X_valid, Y_valid, verbose=2, batch_size=batch_size)\n",
    "print('Logloss损失: %.2f' %(score))\n",
    "print('验证集的准确率 :%.2f'%(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "细节注意：\n",
    "文本数据如何做预处理\n",
    "如何清洗数据\n",
    "对不均衡的类别如何处理\n",
    "文本数据特征工程方式\n",
    "Word2vec/word embedding的理解\n",
    "CNN/LSTM技术细节\n",
    "模型评估与过拟合解决方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
